{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1890b16-e313-47b1-b27e-a9d5e2025153",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 10px; background-color: #64CCC5; margin: 10px; color: #000000; font-family: 'New Times Roman', serif; font-size: 60%; text-align: center; border-radius: 10px; overflow: hidden; font-weight: bold;\"> Question 1: What is an ensemble technique in machine learning?</div>\n",
    "Ensemble techniques in machine learning involve combining the predictions of multiple models to improve overall performance and robustness. The idea is that by aggregating the predictions of multiple models, the weaknesses of individual models can be mitigated, leading to better generalization and more accurate predictions.\n",
    "\n",
    "There are several types of ensemble techniques, and some of the popular ones include:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating):** In bagging, multiple instances of the same learning algorithm are trained on different subsets of the training data. Each model is trained independently, and their predictions are combined through averaging (for regression problems) or voting (for classification problems). Random Forest is an example of a bagging ensemble algorithm.\n",
    "\n",
    "2. **Boosting:** Boosting focuses on training multiple weak learners sequentially, with each model giving more weight to the instances that were misclassified by the previous ones. This way, the ensemble gradually improves its performance. AdaBoost and Gradient Boosting Machines (GBM) are common boosting algorithms.\n",
    "\n",
    "3. **Stacking:** Stacking involves training multiple diverse models and combining their predictions using another model called a meta-learner or blender. The base models' predictions serve as input features for the meta-learner, which then produces the final prediction. Stacking aims to capture the strengths of different models and leverage their complementary abilities.\n",
    "\n",
    "4. **Voting:** In ensemble learning, voting can be used to combine the predictions of multiple models. There are different types of voting, including majority voting, weighted voting, and soft voting. In majority voting, the class predicted by the majority of models is chosen, while in weighted voting, each model's prediction is given a specific weight.\n",
    "\n",
    "Ensemble techniques are powerful tools in machine learning because they can enhance model performance, reduce overfitting, and increase the model's ability to generalize to new, unseen data. However, it's important to note that ensemble methods may also increase computational complexity and training time. The choice of ensemble technique depends on the specific problem and the characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fec6d-a2cf-4bf6-ae2a-772c11fe8b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "895f1d25-7919-4d1e-a262-287d24f07283",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 10px; background-color: #64CCC5; margin: 10px; color: #000000; font-family: 'New Times Roman', serif; font-size: 60%; text-align: center; border-radius: 10px; overflow: hidden; font-weight: bold;\"> Question 2: Why are ensemble techniques used in machine learning?</div>\n",
    "Ensemble techniques are used in machine learning for several reasons, as they offer various advantages that can improve the overall performance and robustness of models. Here are some key reasons why ensemble techniques are commonly employed:\n",
    "\n",
    "1. **Improved Accuracy:** Ensemble methods often lead to higher accuracy compared to individual models. By combining predictions from multiple models, ensemble techniques can mitigate the weaknesses of individual models and capture a more comprehensive understanding of the underlying patterns in the data.\n",
    "\n",
    "2. **Reduced Overfitting:** Ensemble methods can help reduce overfitting, especially in complex models. Overfitting occurs when a model performs well on the training data but fails to generalize to new, unseen data. By combining multiple models, ensemble techniques can provide a more balanced and generalizable solution.\n",
    "\n",
    "3. **Increased Robustness:** Ensemble methods are less sensitive to outliers and noise in the data. Even if one model makes a poor prediction due to noise or outliers, the impact on the overall ensemble is often limited, as other models may compensate for the error.\n",
    "\n",
    "4. **Handling Variability in Data:** Different models may perform well on different subsets of the data or in different regions of the feature space. Ensemble methods, such as bagging and stacking, can harness the diverse strengths of individual models to provide a more reliable and versatile solution across the entire dataset.\n",
    "\n",
    "5. **Model Stability:** Ensemble techniques can enhance the stability of predictions. Since they involve combining multiple models, the overall prediction is less likely to be influenced by fluctuations in the training data or small changes in model parameters.\n",
    "\n",
    "6. **Flexibility:** Ensemble methods are flexible and can be applied to a wide range of machine learning algorithms. They can be used with decision trees, neural networks, support vector machines, and other types of models, making them applicable in various domains.\n",
    "\n",
    "7. **Easy Parallelization:** Some ensemble methods, such as bagging, can be easily parallelized. This makes them well-suited for distributed computing environments, leading to faster training times and improved scalability.\n",
    "\n",
    "8. **Model Interpretability:** Ensemble techniques can sometimes enhance model interpretability. For instance, in the case of decision tree ensembles like Random Forest, it's possible to analyze feature importance and understand which features are more influential in making predictions.\n",
    "\n",
    "While ensemble techniques offer many advantages, it's essential to note that they may also come with increased computational complexity and resource requirements. Additionally, the choice of the specific ensemble method depends on the characteristics of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3489cf98-b2a5-463c-b6fc-590f891503f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6fce048-792f-4acb-bd94-fa2c0cc3cf9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 10px; background-color: #64CCC5; margin: 10px; color: #000000; font-family: 'New Times Roman', serif; font-size: 60%; text-align: center; border-radius: 10px; overflow: hidden; font-weight: bold;\"> Question 3: What is bagging?</div>\n",
    "Bagging, which stands for Bootstrap Aggregating, is an ensemble technique in machine learning that aims to improve the stability and accuracy of a model by training multiple instances of the same learning algorithm on different subsets of the training data. The process involves creating multiple bootstrap samples (random samples with replacement) from the original training dataset and training a model on each sample.\n",
    "\n",
    "Here's a step-by-step explanation of how bagging works:\n",
    "\n",
    "1. **Bootstrap Sampling:** Randomly select subsets of the training data with replacement. This means that some instances may be repeated in a subset, while others may not be included at all. These subsets are referred to as bootstrap samples.\n",
    "\n",
    "2. **Model Training:** Train a base model (often the same learning algorithm) independently on each bootstrap sample. This results in multiple models, each having been exposed to a slightly different subset of the original data.\n",
    "\n",
    "3. **Predictions Aggregation:** When making predictions on new, unseen data, the predictions of each individual model are combined. For regression problems, this usually involves averaging the predictions, and for classification problems, it often involves a majority vote.\n",
    "\n",
    "The key idea behind bagging is to reduce the variance of the model by introducing diversity through the use of different training datasets. By training on multiple subsets, the models are exposed to different perspectives of the data and are less likely to overfit to specific patterns or outliers.\n",
    "\n",
    "Random Forest is a popular ensemble algorithm based on bagging. It uses an ensemble of decision trees, where each tree is trained on a different bootstrap sample. Additionally, Random Forest introduces randomness during the tree-building process by considering a random subset of features at each split, further enhancing diversity among the individual trees.\n",
    "\n",
    "Bagging is a powerful technique for improving the performance and robustness of models, especially when the base models have high variance or are prone to overfitting. It is widely used in practice and has been successfully applied to various machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd11ce2-0eab-4099-acfa-47a5bec4cb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63bbff93-6703-4aa2-8e7e-2be252b99bde",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 10px; background-color: #64CCC5; margin: 10px; color: #000000; font-family: 'New Times Roman', serif; font-size: 60%; text-align: center; border-radius: 10px; overflow: hidden; font-weight: bold;\"> Question 4: What is boosting?</div>\n",
    "Boosting is another ensemble technique in machine learning, but unlike bagging, it focuses on improving the accuracy of a model by combining the strengths of multiple weak learners to create a strong learner. A weak learner is a model that performs slightly better than random chance, and boosting aims to boost its performance by emphasizing instances that were misclassified by the previous models.\n",
    "\n",
    "Here's a general overview of how boosting works:\n",
    "\n",
    "1. **Sequential Training:** Boosting involves training a sequence of weak learners sequentially. Each model is trained to correct the errors made by the previous ones.\n",
    "\n",
    "2. **Instance Weighting:** During training, more emphasis is given to instances that were misclassified by the previous models. The weights of these instances are adjusted to make them more influential in the subsequent model training.\n",
    "\n",
    "3. **Combining Predictions:** The final prediction is obtained by combining the predictions of all the weak learners, typically through a weighted sum.\n",
    "\n",
    "The most common boosting algorithms include:\n",
    "\n",
    "- **AdaBoost (Adaptive Boosting):** In AdaBoost, each weak learner is trained on a weighted version of the training data. The weights are adjusted based on the accuracy of the previous models. Misclassified instances receive higher weights, making them more likely to be correctly classified in the next iteration.\n",
    "\n",
    "- **Gradient Boosting Machines (GBM):** GBM builds trees sequentially, with each tree trained to correct the errors of the previous ones. It minimizes a loss function, often using gradient descent to find the optimal parameters for each new weak learner. XGBoost and LightGBM are popular implementations of gradient boosting.\n",
    "\n",
    "- **Stochastic Gradient Boosting (SGD):** This is a variant of gradient boosting that introduces randomness by subsampling the training data for each iteration. It helps improve training efficiency and generalization.\n",
    "\n",
    "Boosting tends to perform well in practice and is effective when dealing with complex, non-linear relationships in the data. It is particularly useful when the base models are weak but can contribute incrementally to the overall predictive power of the ensemble.\n",
    "\n",
    "While boosting can achieve high accuracy, it is essential to monitor the potential for overfitting, as boosting may become sensitive to noise in the training data, leading to excessively complex models. Regularization techniques and tuning of hyperparameters are often employed to mitigate overfitting in boosting algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd645ea-a25a-4fea-96ad-1255f719b72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6cc306f4-12dc-4560-93e8-10dad25bcc7c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 10px; background-color: #64CCC5; margin: 10px; color: #000000; font-family: 'New Times Roman', serif; font-size: 60%; text-align: center; border-radius: 10px; overflow: hidden; font-weight: bold;\"> Question 5: What are the benefits of using ensemble techniques?</div>\n",
    "Ensemble techniques in machine learning offer several benefits, making them popular and widely used in various applications. Here are some key advantages of using ensemble techniques:\n",
    "\n",
    "1. **Improved Accuracy:** Ensemble methods often lead to higher predictive accuracy compared to individual models. By combining multiple models, the ensemble can mitigate the weaknesses of individual models and provide more robust and accurate predictions.\n",
    "\n",
    "2. **Reduction of Overfitting:** Ensemble techniques can help reduce overfitting, especially in complex models. Overfitting occurs when a model learns the training data too well, including noise and outliers. Ensemble methods, by combining multiple models, can provide a more generalized solution that is less likely to overfit to specific patterns in the data.\n",
    "\n",
    "3. **Increased Robustness:** Ensembles are less sensitive to outliers and noisy data points. Even if one model makes an incorrect prediction for a particular instance, other models in the ensemble may compensate, leading to a more robust overall prediction.\n",
    "\n",
    "4. **Handling Variability in Data:** Different models may perform well on different subsets of the data or in different regions of the feature space. Ensemble techniques, such as bagging and stacking, can harness the diverse strengths of individual models to provide a more reliable and versatile solution across the entire dataset.\n",
    "\n",
    "5. **Model Generalization:** Ensemble methods improve the generalization capability of models. By combining predictions from multiple models trained on different subsets of the data, the ensemble is better equipped to capture underlying patterns that may be missed by individual models.\n",
    "\n",
    "6. **Model Interpretability:** Some ensemble methods, such as Random Forest, provide insights into feature importance. This can aid in understanding which features are more influential in making predictions, contributing to better model interpretability.\n",
    "\n",
    "7. **Flexibility:** Ensemble methods are versatile and can be applied to a wide range of machine learning algorithms. They are not limited to specific types of models, making them suitable for various problems and domains.\n",
    "\n",
    "8. **Easy Parallelization:** Certain ensemble methods, like bagging, can be easily parallelized, allowing for faster training times and improved scalability in distributed computing environments.\n",
    "\n",
    "9. **Applicability to Different Problems:** Ensemble techniques are effective for both classification and regression problems. They can be applied to a diverse set of machine learning algorithms, including decision trees, support vector machines, neural networks, and more.\n",
    "\n",
    "Despite these advantages, it's important to note that ensemble methods may also come with increased computational complexity and resource requirements. Additionally, the choice of the specific ensemble method depends on the characteristics of the data and the problem at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296248b8-f3f2-4df4-b901-e999e8bff565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a02164c-0b4a-4254-b7b5-cffa48b6e5bd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 10px; background-color: #64CCC5; margin: 10px; color: #000000; font-family: 'New Times Roman', serif; font-size: 60%; text-align: center; border-radius: 10px; overflow: hidden; font-weight: bold;\"> Question 6: Are ensemble techniques always better than individual models?</div>\n",
    "While ensemble techniques often lead to improved performance and robustness, they are not always guaranteed to be better than individual models. The effectiveness of ensemble methods depends on various factors, and there are situations where using an ensemble may not provide significant benefits. Here are some considerations:\n",
    "\n",
    "1. **Quality of Base Models:** The success of an ensemble relies on the diversity and quality of the base models. If the base models are weak, highly correlated, or consistently make similar errors, the ensemble might not perform significantly better than the best individual model.\n",
    "\n",
    "2. **Computational Resources:** Ensembling multiple models can increase computational requirements, including training time and memory usage. In situations where computational resources are limited, using a single well-tuned model might be more practical.\n",
    "\n",
    "3. **Data Size and Quality:** For small datasets, ensemble techniques may not be as effective, as creating diverse subsets for training individual models becomes challenging. Additionally, if the dataset is of low quality or contains significant noise, ensembling might amplify these issues rather than mitigate them.\n",
    "\n",
    "4. **Overfitting Risk:** While ensemble methods can reduce overfitting in many cases, there is a risk that they may overfit to the noise in the training data, particularly if the ensemble becomes too complex. Regularization techniques and careful hyperparameter tuning are necessary to mitigate this risk.\n",
    "\n",
    "5. **Interpretability:** Ensembles can be more challenging to interpret compared to individual models, especially when using complex techniques like stacking. In situations where model interpretability is a critical requirement, a single, interpretable model might be preferred.\n",
    "\n",
    "6. **Domain-Specific Considerations:** Some specific domains or applications may have constraints or requirements that make ensemble methods less suitable. For example, in real-time applications with tight latency requirements, the additional computational cost of ensembling may be a limiting factor.\n",
    "\n",
    "7. **Imbalanced Datasets:** In cases of imbalanced datasets, where one class is underrepresented, ensembling may not always guarantee improvements. Some ensemble methods may be biased towards the majority class, and addressing class imbalance might require additional techniques.\n",
    "\n",
    "It's essential to carefully assess the characteristics of the problem at hand, the quality of available data, and the nature of the models being used before deciding to employ ensemble techniques. In many cases, ensembling can provide substantial benefits, but there are scenarios where a well-tuned individual model might be more practical or even preferable. It's often a matter of experimentation and empirical validation to determine the most effective approach for a specific machine learning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9ef337-abb7-475e-bc53-db1793365d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ad3f3f0-f038-4d23-9f44-b35078151f09",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 10px; background-color: #64CCC5; margin: 10px; color: #000000; font-family: 'New Times Roman', serif; font-size: 60%; text-align: center; border-radius: 10px; overflow: hidden; font-weight: bold;\"> Question 7: How is the confidence interval calculated using bootstrap?</div>\n",
    "The bootstrap method is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling with replacement from the observed data. It can be used to calculate confidence intervals for various statistics, including the mean, median, standard deviation, and more.\n",
    "\n",
    "Here's a general outline of how the bootstrap method can be used to calculate a confidence interval:\n",
    "\n",
    "1. **Bootstrap Sampling:**\n",
    "   - Randomly draw a sample (with replacement) from the observed data. The size of the bootstrap sample is typically the same as the size of the original dataset.\n",
    "   - Repeat this process to create multiple bootstrap samples (often several thousand).\n",
    "\n",
    "2. **Calculate Statistic:**\n",
    "   - For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, standard deviation, etc.). This gives you a distribution of the statistic.\n",
    "\n",
    "3. **Confidence Interval:**\n",
    "   - Calculate the desired quantiles of the distribution of the statistic to obtain the confidence interval. The lower and upper bounds of the interval correspond to the chosen quantiles.\n",
    "\n",
    "For example, to calculate a 95% confidence interval using the bootstrap method, you would typically use the 2.5th and 97.5th percentiles of the bootstrap distribution as the lower and upper bounds, respectively.\n",
    "\n",
    "Here's a step-by-step breakdown:\n",
    "\n",
    "- Sort the bootstrap distribution.\n",
    "- Find the value at the 2.5th percentile (lower bound).\n",
    "- Find the value at the 97.5th percentile (upper bound).\n",
    "\n",
    "The range between the lower and upper bounds represents the 95% confidence interval.\n",
    "\n",
    "In mathematical terms, if $(B)$ is the number of bootstrap samples, and $(q_1)$ and $(q_2)$ are the desired quantiles (e.g., 2.5% and 97.5% for a 95% confidence interval), the confidence interval is given by:\n",
    "\n",
    "$$[ \\left( \\text{Quantile}_{q_1}, \\text{Quantile}_{q_2} \\right) ]$$\n",
    "\n",
    "The bootstrap method is versatile and can be applied to various statistics, making it a powerful tool for estimating the uncertainty associated with a particular parameter or model performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e3c0b-f454-4935-aac9-871de1ba21b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "768f9eb9-7b23-49ee-b2b2-6d2ffca82bb8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 10px; background-color: #64CCC5; margin: 10px; color: #000000; font-family: 'New Times Roman', serif; font-size: 60%; text-align: center; border-radius: 10px; overflow: hidden; font-weight: bold;\"> Question 8: How does bootstrap work and What are the steps involved in bootstrap?</div>\n",
    "\n",
    "Bootstrap is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly resampling with replacement from the observed data. The goal is to create multiple datasets (bootstrap samples) that simulate the variability in the original data. This allows for the estimation of standard errors, confidence intervals, and other statistical properties of a given estimator or model. Here are the basic steps involved in the bootstrap procedure:\n",
    "\n",
    "1. **Original Data:**\n",
    "   - Begin with a dataset containing \\(n\\) observations.\n",
    "\n",
    "2. **Random Sampling with Replacement:**\n",
    "   - Randomly draw \\(n\\) samples (with replacement) from the original dataset to create a new dataset, known as a bootstrap sample.\n",
    "   - The size of each bootstrap sample is the same as the size of the original dataset.\n",
    "\n",
    "3. **Calculate Statistic:**\n",
    "   - Calculate the statistic of interest (e.g., mean, median, standard deviation, regression coefficients, etc.) for the current bootstrap sample.\n",
    "   - This step involves applying the estimator or model to the resampled data.\n",
    "\n",
    "4. **Repeat Steps 2-3:**\n",
    "   - Repeat steps 2 and 3 a large number of times (typically thousands of times) to create multiple bootstrap samples and compute the corresponding statistics.\n",
    "\n",
    "5. **Estimate Statistic Distribution:**\n",
    "   - Create a distribution of the calculated statistics from the bootstrap samples.\n",
    "   - This distribution represents the variability of the statistic and can be used to estimate standard errors, construct confidence intervals, and assess the uncertainty associated with the estimator.\n",
    "\n",
    "6. **Calculate Confidence Intervals (Optional):**\n",
    "   - If the goal is to calculate a confidence interval for the statistic, identify the desired quantiles of the distribution.\n",
    "   - For example, a 95% confidence interval would involve finding the 2.5th and 97.5th percentiles of the bootstrap distribution.\n",
    "\n",
    "The key idea behind bootstrap is that, by resampling with replacement from the observed data, you simulate the process of drawing multiple samples from the underlying population. This allows you to empirically approximate the distribution of a statistic without making strong parametric assumptions about the population distribution.\n",
    "\n",
    "Bootstrap can be applied to a wide range of statistical problems, and its simplicity and flexibility make it a valuable tool in statistical analysis and machine learning. It is particularly useful when the underlying distribution of the data is unknown or complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ed0f6-ed7f-4807-9f26-3bd17155f79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4655389b-56c7-4483-a016-4837d39cb687",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 10px; background-color: #64CCC5; margin: 10px; color: #000000; font-family: 'New Times Roman', serif; font-size: 60%; text-align: center; border-radius: 10px; overflow: hidden; font-weight: bold;\"> Question 9:A researcher wants to estimate the mean height of a population of trees. They measure the height of a sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use bootstrap to estimate the 95% confidence interval for the population mean height.</div>\n",
    "\n",
    "To estimate the 95% confidence interval for the population mean height using the bootstrap method, you would follow these steps:\n",
    "\n",
    "1. **Original Data:**\n",
    "   - You have a sample of 50 tree heights with a mean of 15 meters and a standard deviation of 2 meters.\n",
    "\n",
    "2. **Bootstrap Sampling:**\n",
    "   - Randomly draw 50 samples (with replacement) from the original dataset to create a new dataset, known as a bootstrap sample.\n",
    "   - Repeat this process a large number of times (e.g., 10,000 iterations).\n",
    "\n",
    "3. **Calculate Mean Height for Each Bootstrap Sample:**\n",
    "   - For each bootstrap sample, calculate the mean height.\n",
    "\n",
    "4. **Estimate Distribution of the Mean Height:**\n",
    "   - Create a distribution of the calculated means from the bootstrap samples.\n",
    "\n",
    "5. **Calculate Confidence Intervals:**\n",
    "   - Identify the 2.5th and 97.5th percentiles of the distribution. This forms the 95% confidence interval for the population mean height.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d305577a-7de6-45d6-9ca4-e883592a3a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrap 95% Confidence Interval for Mean Height: [14.43472829 15.55905269]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Here's a Python code snippet using a simple simulation to demonstrate the bootstrap procedure:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "sample_mean = 15\n",
    "sample_std = 2\n",
    "sample_size = 50\n",
    "num_bootstrap_samples = 10000\n",
    "\n",
    "# Generate bootstrap samples\n",
    "bootstrap_samples = np.random.normal(loc=sample_mean, scale=sample_std, size=(num_bootstrap_samples, sample_size))\n",
    "\n",
    "# Calculate mean height for each bootstrap sample\n",
    "bootstrap_means = np.mean(bootstrap_samples, axis=1)\n",
    "\n",
    "# Calculate 95% confidence interval\n",
    "confidence_interval = np.percentile(bootstrap_means, [2.5, 97.5])\n",
    "\n",
    "print(\"Bootstrap 95% Confidence Interval for Mean Height:\", confidence_interval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2077aec-c9b4-4e62-aff8-d232b8e01736",
   "metadata": {},
   "source": [
    "In this example, `np.random.normal` is used to generate bootstrap samples assuming a normal distribution of tree heights. The resulting `bootstrap_means` array contains the mean height for each bootstrap sample. Finally, the 2.5th and 97.5th percentiles of this distribution are calculated to obtain the 95% confidence interval.\n",
    "\n",
    "Keep in mind that this is a simple demonstration, and in practice, you would use the actual observed data for the bootstrap sampling. Additionally, the accuracy of the bootstrap estimate improves as the number of bootstrap samples increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df23ba-af27-4bff-84e5-3034f5f4e092",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <div style=\"padding: 15px; background-color: #D2E0FB; margin: 15px; color: #000000; font-family: 'New Times Roman', serif; font-size: 110%; text-align: center; border-radius: 10px; overflow: hidden; font-weight: bold;\"> ***...Complete...***</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
