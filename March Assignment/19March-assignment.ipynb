{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1890b16-e313-47b1-b27e-a9d5e2025153",
   "metadata": {},
   "source": [
    "`Question 1`. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "`Answer` :### Min-Max Scaling\n",
    "\n",
    "Min-Max scaling, also known as min-max normalization or feature scaling, is a data preprocessing technique commonly used in machine learning and data analysis. Its purpose is to transform the values of numerical features (variables) in a dataset to a specific range, typically between 0 and 1. This scaling technique helps standardize the data and ensures that all features have the same scale, making it easier for machine learning algorithms to converge during training and preventing features with larger value ranges from dominating the learning process.\n",
    "\n",
    "The Min-Max scaling process involves the following steps:\n",
    "\n",
    "1. Identify the range for scaling: Determine the desired minimum and maximum values for the transformed data, typically 0 and 1. These values can be adjusted to a different range if necessary, but the 0 to 1 range is the most common choice.\n",
    "\n",
    "2. For each feature (column) in your dataset:\n",
    "   - Find the minimum value (min) and the maximum value (max) in that feature.\n",
    "\n",
    "3. For each data point in the feature:\n",
    "   - Apply the following formula to scale the data:\n",
    "   \n",
    "     Scaled_Value = (Original_Value - Min) / (Max - Min)\n",
    "\n",
    "   - The scaled value will now fall within the specified range (e.g., 0 to 1).\n",
    "\n",
    "Min-Max scaling ensures that the minimum value of the original feature will be transformed to 0, the maximum value to 1, and all other values will be proportionally scaled in between. It preserves the relative relationships between data points and does not change the distribution of the data.\n",
    "\n",
    "Min-Max scaling is particularly useful when working with machine learning algorithms that are sensitive to the scale of features, such as k-nearest neighbors and support vector machines. It is essential to apply this technique during the data preprocessing phase to improve the model's performance and avoid issues related to different feature scales.\n",
    "\n",
    "Keep in mind that Min-Max scaling may not be suitable for all situations, especially when dealing with outliers or when the data distribution is highly skewed. In such cases, other scaling methods like Z-score scaling (Standardization) might be more appropriate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6eba20b-9bd1-490d-9748-0a0a83e6f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Example\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53d82d0-950c-44ff-936c-8127b088f277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup</th>\n",
       "      <th>dropoff</th>\n",
       "      <th>passengers</th>\n",
       "      <th>distance</th>\n",
       "      <th>fare</th>\n",
       "      <th>tip</th>\n",
       "      <th>tolls</th>\n",
       "      <th>total</th>\n",
       "      <th>color</th>\n",
       "      <th>payment</th>\n",
       "      <th>pickup_zone</th>\n",
       "      <th>dropoff_zone</th>\n",
       "      <th>pickup_borough</th>\n",
       "      <th>dropoff_borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-23 20:21:09</td>\n",
       "      <td>2019-03-23 20:27:24</td>\n",
       "      <td>1</td>\n",
       "      <td>1.60</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.95</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Lenox Hill West</td>\n",
       "      <td>UN/Turtle Bay South</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-04 16:11:55</td>\n",
       "      <td>2019-03-04 16:19:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.79</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.30</td>\n",
       "      <td>yellow</td>\n",
       "      <td>cash</td>\n",
       "      <td>Upper West Side South</td>\n",
       "      <td>Upper West Side South</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pickup             dropoff  passengers  distance  fare   tip  \\\n",
       "0 2019-03-23 20:21:09 2019-03-23 20:27:24           1      1.60   7.0  2.15   \n",
       "1 2019-03-04 16:11:55 2019-03-04 16:19:00           1      0.79   5.0  0.00   \n",
       "\n",
       "   tolls  total   color      payment            pickup_zone  \\\n",
       "0    0.0  12.95  yellow  credit card        Lenox Hill West   \n",
       "1    0.0   9.30  yellow         cash  Upper West Side South   \n",
       "\n",
       "            dropoff_zone pickup_borough dropoff_borough  \n",
       "0    UN/Turtle Bay South      Manhattan       Manhattan  \n",
       "1  Upper West Side South      Manhattan       Manhattan  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset('taxis')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51a4a461-984a-405b-ab31-5d7a81a54446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup</th>\n",
       "      <th>dropoff</th>\n",
       "      <th>passengers</th>\n",
       "      <th>distance</th>\n",
       "      <th>fare</th>\n",
       "      <th>tip</th>\n",
       "      <th>tolls</th>\n",
       "      <th>total</th>\n",
       "      <th>color</th>\n",
       "      <th>payment</th>\n",
       "      <th>pickup_zone</th>\n",
       "      <th>dropoff_zone</th>\n",
       "      <th>pickup_borough</th>\n",
       "      <th>dropoff_borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-03-23 20:21:09</td>\n",
       "      <td>2019-03-23 20:27:24</td>\n",
       "      <td>1</td>\n",
       "      <td>0.043597</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.064759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.067139</td>\n",
       "      <td>yellow</td>\n",
       "      <td>credit card</td>\n",
       "      <td>Lenox Hill West</td>\n",
       "      <td>UN/Turtle Bay South</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-03-04 16:11:55</td>\n",
       "      <td>2019-03-04 16:19:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.021526</td>\n",
       "      <td>0.026846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.046104</td>\n",
       "      <td>yellow</td>\n",
       "      <td>cash</td>\n",
       "      <td>Upper West Side South</td>\n",
       "      <td>Upper West Side South</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               pickup             dropoff  passengers  distance      fare  \\\n",
       "0 2019-03-23 20:21:09 2019-03-23 20:27:24           1  0.043597  0.040268   \n",
       "1 2019-03-04 16:11:55 2019-03-04 16:19:00           1  0.021526  0.026846   \n",
       "\n",
       "        tip  tolls     total   color      payment            pickup_zone  \\\n",
       "0  0.064759    0.0  0.067139  yellow  credit card        Lenox Hill West   \n",
       "1  0.000000    0.0  0.046104  yellow         cash  Upper West Side South   \n",
       "\n",
       "            dropoff_zone pickup_borough dropoff_borough  \n",
       "0    UN/Turtle Bay South      Manhattan       Manhattan  \n",
       "1  Upper West Side South      Manhattan       Manhattan  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using sklearnl i scaled the colums ['distance', 'fare', 'tip', 'tolls', 'total']  btw 0-1.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max= MinMaxScaler()\n",
    "scaled_data= pd.DataFrame(min_max.fit_transform(df[[ 'distance', 'fare', 'tip', 'tolls','total']]),columns= ['distance', 'fare', 'tip', 'tolls','total'],)\n",
    "df[['distance', 'fare', 'tip', 'tolls', 'total']] =scaled_data\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986b440-37eb-4e47-ac3d-81091c873c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1783352d-152c-499c-bb55-165eaf4cede6",
   "metadata": {},
   "source": [
    "`Question 2`. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "`Answer` :### Unit Vector Scaling vs. Min-Max Scaling\n",
    "\n",
    "Unit Vector scaling and Min-Max scaling are two different techniques used in data preprocessing to normalize the values of features. While both methods aim to bring features to a common scale, they differ in how they achieve this.\n",
    "\n",
    "#### Min-Max Scaling\n",
    "\n",
    "Min-Max scaling, also known as min-max normalization, scales the values of features to a specific range, typically between 0 and 1. It follows these steps:\n",
    "\n",
    "1. Identify the desired minimum and maximum values for scaling, often set to 0 and 1.\n",
    "2. Find the minimum and maximum values of the feature.\n",
    "3. Scale the feature values using the formula:\n",
    "\n",
    "   Scaled_Value = (Original_Value - Min) / (Max - Min)\n",
    "\n",
    "Min-Max scaling is widely used and ensures that all feature values fall within the specified range. It is particularly useful when you want to preserve the relationships between the original data points.\n",
    "\n",
    "#### Unit Vector Scaling\n",
    "\n",
    "Unit Vector scaling, also known as vector normalization, scales the features in such a way that they are transformed into unit vectors. A unit vector has a length of 1 and points in the same direction as the original vector. This scaling method is commonly used in machine learning algorithms that rely on the magnitude of feature vectors, such as support vector machines. The steps for unit vector scaling are as follows:\n",
    "\n",
    "1. Compute the magnitude (length) of the feature vector, which is the square root of the sum of the squares of individual feature values.\n",
    "2. Divide each feature value by the magnitude of the vector.\n",
    "\n",
    "   Scaled_Value = Original_Value / Magnitude_of_Vector\n",
    "\n",
    "Unit Vector scaling ensures that all feature vectors have a length of 1, preserving the direction of the vectors while standardizing their magnitude. It is particularly useful when the direction of the feature vectors is more important than their absolute values.\n",
    "\n",
    "In summary, the key difference between Min-Max scaling and Unit Vector scaling is the objective: Min-Max scaling aims to bring values within a specific range, while Unit Vector scaling standardizes the magnitude while preserving the direction of feature vectors. The choice between the two methods depends on the requirements of your specific application and the nature of your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2101c198-a7e6-42a8-a8a6-e2e0294722ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "##Example\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fba2c42f-dfa5-4de4-81e9-872869d0e0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   carat      cut color clarity  depth  table  price     x     y     z\n",
       "0   0.23    Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1   0.21  Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset('diamonds')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e8aa9eb-8edf-482d-bbe0-bc882ca6acaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000684</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>0.182854</td>\n",
       "      <td>0.163528</td>\n",
       "      <td>0.969274</td>\n",
       "      <td>0.011744</td>\n",
       "      <td>0.011833</td>\n",
       "      <td>0.007225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000623</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>0.177417</td>\n",
       "      <td>0.180978</td>\n",
       "      <td>0.967192</td>\n",
       "      <td>0.011541</td>\n",
       "      <td>0.011393</td>\n",
       "      <td>0.006853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      carat      cut color clarity     depth     table     price         x  \\\n",
       "0  0.000684    Ideal     E     SI2  0.182854  0.163528  0.969274  0.011744   \n",
       "1  0.000623  Premium     E     SI1  0.177417  0.180978  0.967192  0.011541   \n",
       "\n",
       "          y         z  \n",
       "0  0.011833  0.007225  \n",
       "1  0.011393  0.006853  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using sklearnl normalize, i scaled the colums ['carat\t','depth', 'table', 'price', 'x', 'y', 'z']  btw 0-1.\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "scaled_data= pd.DataFrame(normalize(df[['carat','depth', 'table', 'price', 'x', 'y', 'z']]),columns= ['carat','depth', 'table', 'price', 'x', 'y', 'z'],)\n",
    "df[['carat','depth', 'table', 'price', 'x', 'y', 'z']] =scaled_data\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf04afb-a034-4118-89ed-8e4b4d1d4a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0efae261-f84d-483b-9c40-62c64c95ee02",
   "metadata": {},
   "source": [
    "`Question 3`. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "`Answer` :### PCA (Principal Component Analysis) for Dimensionality Reduction\n",
    "\n",
    "**PCA (Principal Component Analysis)** is a widely used statistical technique for reducing the dimensionality of data while preserving as much of the variability in the data as possible. It does this by transforming the original data into a new coordinate system where the dimensions (principal components) are linear combinations of the original features. The first principal component explains the most variance, the second principal component explains the second most variance, and so on. By selecting a subset of these principal components, you can effectively reduce the dimensionality of your data.\n",
    "\n",
    "#### How PCA Works\n",
    "\n",
    "1. **Standardize the Data:** First, you should standardize your data by centering it (subtracting the mean) and scaling it (dividing by the standard deviation) to ensure that features have comparable scales.\n",
    "\n",
    "2. **Compute the Covariance Matrix:** Next, calculate the covariance matrix of the standardized data. This matrix describes the relationships between features.\n",
    "\n",
    "3. **Eigendecomposition:** Perform an eigendecomposition of the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "\n",
    "4. **Select Principal Components:** The eigenvectors represent the principal components, and the eigenvalues indicate the amount of variance explained by each component. By sorting the eigenvalues in descending order, you can select the top k components that capture the most variance in the data.\n",
    "\n",
    "5. **Transform the Data:** Transform the original data into the new coordinate system formed by the selected principal components.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let's illustrate PCA with an example using Python and the scikit-learn library:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Create sample data\n",
    "data = np.random.rand(100, 3)  # 100 data points with 3 features each\n",
    "\n",
    "# Standardize the data\n",
    "mean = np.mean(data, axis=0)\n",
    "std_dev = np.std(data, axis=0)\n",
    "standardized_data = (data - mean) / std_dev\n",
    "\n",
    "# Create a PCA instance and specify the number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA on the standardized data\n",
    "pca.fit(standardized_data)\n",
    "\n",
    "# Transform the data into the reduced dimension space\n",
    "reduced_data = pca.transform(standardized_data)\n",
    "\n",
    "print(\"Original Data Shape:\", standardized_data.shape)\n",
    "print(\"Reduced Data Shape:\", reduced_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723deb9a-8a1f-442c-8db6-29ada6ff6b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39441cce-f62b-4fc6-9f12-6848006a338d",
   "metadata": {},
   "source": [
    "`Question 4`. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "`Answer` :### PCA for Feature Extraction\n",
    "\n",
    "**PCA (Principal Component Analysis)** is not only a technique for dimensionality reduction but also a method for feature extraction. Feature extraction refers to the process of selecting a subset of relevant features from the original feature set. PCA can be used for feature extraction by identifying and retaining the most informative features while discarding the less important ones.\n",
    "\n",
    "#### Relationship between PCA and Feature Extraction\n",
    "\n",
    "The relationship between PCA and feature extraction lies in the way PCA identifies the most significant dimensions (principal components) in the data. These principal components are linear combinations of the original features, and they capture the most variance in the data. By selecting a subset of these principal components, you effectively extract a reduced set of features that represent the data with reduced dimensionality.\n",
    "\n",
    "#### How PCA Can Be Used for Feature Extraction\n",
    "\n",
    "1. **Standardize the Data:** As a first step, standardize your data by centering it (subtracting the mean) and scaling it (dividing by the standard deviation) to ensure that features have comparable scales.\n",
    "\n",
    "2. **Compute PCA:** Apply PCA to the standardized data. The eigenvalues and eigenvectors of the covariance matrix will reveal the importance of each principal component.\n",
    "\n",
    "3. **Select Principal Components:** You can choose to keep a subset of the principal components based on the proportion of variance they explain. Retaining the top-k principal components will effectively extract a reduced set of features.\n",
    "\n",
    "4. **Transform the Data:** Transform the data into the new coordinate system formed by the selected principal components. These transformed values can be considered as the extracted features.\n",
    "\n",
    "#### Example\n",
    "\n",
    "Let's illustrate PCA for feature extraction with an example using Python and the scikit-learn library:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Create sample data\n",
    "data = np.random.rand(100, 5)  # 100 data points with 5 features each\n",
    "\n",
    "# Standardize the data\n",
    "mean = np.mean(data, axis=0)\n",
    "std_dev = np.std(data, axis=0)\n",
    "standardized_data = (data - mean) / std_dev\n",
    "\n",
    "# Create a PCA instance\n",
    "pca = PCA(n_components=2)  # Extract 2 features\n",
    "\n",
    "# Fit PCA on the standardized data\n",
    "pca.fit(standardized_data)\n",
    "\n",
    "# Transform the data into the reduced feature space\n",
    "extracted_features = pca.transform(standardized_data)\n",
    "\n",
    "print(\"Original Data Shape:\", standardized_data.shape)\n",
    "print(\"Extracted Features Shape:\", extracted_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3126d7-5a7d-4c73-8a55-eed9606e12c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e6e57de-99f7-45e3-bd1b-f5ae27ce0ed0",
   "metadata": {},
   "source": [
    "`Question 5`. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "`Answer` :\n",
    "```python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your dataset into a pandas DataFrame (replace 'data.csv' food delivery servic data file)\n",
    "df = pd.read_csv('data.csv')\n",
    "\n",
    "# Create a Min-Max Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Select the features to be scaled\n",
    "features_to_scale = ['price', 'rating', 'delivery_time']\n",
    "\n",
    "# Fit the scaler on the selected features and transform the data\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "\n",
    "# Fit the scaler on the selected features and transform the data\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d95d53-c30b-4ae8-b0db-c1da9489dbd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc6193fa-ca0f-407d-83d4-c7d1a637d3e1",
   "metadata": {},
   "source": [
    "`Question 6`. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "`Answer` :\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load and prepare your dataset\n",
    "# Replace 'stock_data.csv' with the path to your dataset\n",
    "data = pd.read_csv('stock_data.csv')\n",
    "\n",
    "# Select relevant features from your dataset (financial data, market trends, etc.)\n",
    "# You can modify this list to include the features you want to consider\n",
    "selected_features = ['feature1', 'feature2', 'feature3', ...]\n",
    "\n",
    "# Create a DataFrame with the selected features\n",
    "selected_data = data[selected_features]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(selected_data)\n",
    "\n",
    "# Create a PCA instance\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA on the standardized data\n",
    "pca.fit(standardized_data)\n",
    "\n",
    "# Determine the number of components to retain\n",
    "# Typically, you can set a threshold for explained variance or choose a fixed number of components\n",
    "# For example, to retain 95% of the variance, you can use:\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance_ratio.cumsum()\n",
    "num_components_to_retain = len(cumulative_variance[cumulative_variance < 0.95]) + 1\n",
    "\n",
    "# Apply PCA with the selected number of components\n",
    "pca = PCA(n_components=num_components_to_retain)\n",
    "reduced_data = pca.fit_transform(standardized_data)\n",
    "\n",
    "# Your 'reduced_data' now contains the transformed data with reduced dimensionality\n",
    "# You can use this reduced data to build and train your stock price prediction model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21d1156-d23e-4cab-8f8f-a4f9a0ff7eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2290e8a4-aed8-4f2c-968e-cfb7df4744d4",
   "metadata": {},
   "source": [
    "`Question 7`. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n",
    "\n",
    "`Answer` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16017788-b112-4cd5-99fa-d7e8f64dd437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled Data:\n",
      "   Data  Sacled data\n",
      "0     1     0.000000\n",
      "1     5     0.210526\n",
      "2    10     0.473684\n",
      "3    15     0.736842\n",
      "4    20     1.000000\n"
     ]
    }
   ],
   "source": [
    "df_q7 = pd.DataFrame([1, 5, 10, 15, 20], columns=['Data'])\n",
    "\n",
    "# Create a Min-Max Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Select the features to be scaled\n",
    "features_to_scale = ['Data']\n",
    "\n",
    "# Fit the scaler on the selected features and transform the data\n",
    "df_q7['Sacled data'] = scaler.fit_transform(df_q7[features_to_scale])\n",
    "\n",
    "print(\"Scaled Data:\")\n",
    "print(df_q7.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc02ac-b691-465e-b0ba-db5c3e3ccd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c15655a5-43f4-41cc-a119-ec160182e120",
   "metadata": {},
   "source": [
    "`Question 8`. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "`Answer` :\n",
    "\n",
    "1. Import the necessary libraries, including PCA and StandardScaler.\n",
    "2. Load your dataset and select the numeric features (excluding 'gender') for PCA.\n",
    "3. Standardize the data to ensure all features have comparable scales.\n",
    "4. Create a PCA instance and fit it on the standardized data.\n",
    "5. Calculate the cumulative explained variance.\n",
    "6. Determine the number of components to retain based on the threshold of 95% of the total variance.\n",
    "7. Apply PCA with the selected number of components to reduce the dimensionality of your dataset.\n",
    "\n",
    "The chosen number of components to retain (in this case, num_components_to_retain) is based on the 95% threshold for explained variance, ensuring that the majority of the variance in the data is retained while reducing dimensionality. You can adjust the threshold as needed based on your specific project requirements.\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the path to your dataset)\n",
    "data = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Select the numeric features to be used in PCA (excluding 'gender')\n",
    "numeric_features = ['height', 'weight', 'age', 'blood_pressure']\n",
    "\n",
    "# Create a DataFrame with the selected features\n",
    "selected_data = data[numeric_features]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(selected_data)\n",
    "\n",
    "# Create a PCA instance\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA on the standardized data\n",
    "pca.fit(standardized_data)\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = explained_variance_ratio.cumsum()\n",
    "\n",
    "# Determine the number of components to retain for 95% of the total variance\n",
    "threshold = 0.95  # Set the desired explained variance threshold (95%)\n",
    "num_components_to_retain = len(cumulative_variance[cumulative_variance < threshold]) + 1\n",
    "\n",
    "# Apply PCA with the selected number of components\n",
    "pca = PCA(n_components=num_components_to_retain)\n",
    "reduced_data = pca.fit_transform(standardized_data)\n",
    "\n",
    "# The 'reduced_data' now contains the transformed data with reduced dimensionality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f06b423-67cb-445c-a60a-b66948f42d2c",
   "metadata": {},
   "source": [
    "## Complete..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
